{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "15e06953",
      "metadata": {
        "id": "15e06953"
      },
      "source": [
        "\n",
        "Text Preprocessing & Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f420fedc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f420fedc",
        "outputId": "098eec27-12aa-47db-c990-bddf3cab1469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Sentences:\n",
            "['punjab-based', 'self-styled', 'preacher', 'bajinder', 'singh', 'found', 'himself', 'in', 'fresh', 'controversy', 'after', 'a', 'video', 'allegedly', 'showing', 'him', 'assaulting', 'a', 'woman', 'surfaced', 'online', '.']\n",
            "['the', 'footage', 'of', 'the', 'incident', 'has', 'now', 'gone', 'viral', '.']\n",
            "['it', 'shows', 'singh', ',', 'who', 'is', 'involved', 'in', 'multiple', 'sexual', 'harassment', 'cases', ',', 'throwing', 'a', 'pile', 'of', 'papers', 'at', 'a', 'woman', 'seated', 'with', 'a', 'child', 'in', 'his', 'office', '.']\n",
            "['moments', 'later', ',', 'as', 'she', 'approaches', 'him', ',', 'he', 'appears', 'to', 'push', 'her', '.']\n",
            "['the', 'situation', 'quickly', 'escalates', ',', 'with', 'others', 'in', 'the', 'room', 'intervening', 'to', 'prevent', 'a', 'further', 'altercation', '.']\n",
            "['the', 'video', ',', 'shared', 'by', 'journalist', 'gagandeep', 'singh', ',', 'ends', 'as', 'both', 'of', 'them', 'were', 'seen', 'continuing', 'their', 'heated', 'exchange', '.']\n",
            "['the', 'new', 'video', 'came', 'a', 'week', 'after', 'singh', 'appeared', 'in', 'a', 'mohali', 'court', 'alongside', 'six', 'others', 'in', 'connection', 'with', 'a', '2018', 'sexual', 'harassment', 'case', '.']\n",
            "['on', 'march', '3', ',', 'non-bailable', 'warrants', 'were', 'issued', 'against', 'him', '.']\n",
            "['singh', ',', 'who', 'runs', 'a', 'church', 'in', 'majri', ',', 'was', 'first', 'arrested', 'on', 'july', '20', ',', '2018', ',', 'at', 'delhi', 'airport', 'while', 'attempting', 'to', 'board', 'a', 'flight', 'to', 'london', '.']\n",
            "['he', 'is', 'accused', 'of', 'raping', 'a', 'woman', 'from', 'zirakpur', ',', 'who', 'had', 'alleged', 'that', 'singh', 'lured', 'her', 'into', 'his', 'religious', 'circle', 'in', '2017', 'before', 'sexually', 'assaulting', 'her', 'at', 'his', 'mohali', 'residence', '.']\n",
            "['according', 'to', 'police', ',', 'he', 'also', 'recorded', 'the', 'act', 'and', 'later', 'used', 'the', 'footage', 'to', 'blackmail', 'her', 'into', 'silence', '.']\n",
            "['she', 'eventually', 'lodged', 'a', 'complaint', 'in', 'april', '2018', 'after', 'which', 'singh', 'went', 'into', 'hiding', '.']\n",
            "['more', 'trouble', 'followed', 'on', 'march', '2', 'this', 'year', ',', 'when', 'a', 'separate', 'fir', 'for', 'sexual', 'harassment', 'was', 'filed', 'against', 'singh', 'in', 'jalandhar', '.']\n",
            "['while', 'he', 'remains', 'out', 'on', 'bail', 'in', 'the', 'zirakpur', 'case', ',', 'he', 'has', 'yet', 'to', 'be', 'taken', 'into', 'custody', 'for', 'the', 'latest', 'charges', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab',quiet=True)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "text = input(\"Please enter your news article:\")\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sent.lower()) for sent in sentences]\n",
        "\n",
        "print(\"Tokenized Sentences:\")\n",
        "for sentence in tokenized_sentences:\n",
        "    print(sentence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb912e9",
      "metadata": {
        "id": "8fb912e9"
      },
      "source": [
        "## Phase 3: Sentence Ranking using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b2af3109",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2af3109",
        "outputId": "ab86b90a-ffe2-4776-9fb9-143e037751eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Summary:\n",
            "He is accused of raping a woman from Zirakpur, who had alleged that Singh lured her into his religious circle in 2017 before sexually assaulting her at his Mohali residence. It shows Singh, who is involved in multiple sexual harassment cases, throwing a pile of papers at a woman seated with a child in his office. Singh, who runs a church in Majri, was first arrested on July 20, 2018, at Delhi airport while attempting to board a flight to London. \n",
            "\n",
            "Word Frequencies:\n",
            "a: 6 times\n",
            "in: 4 times\n",
            "who: 3 times\n",
            "singh: 3 times\n",
            "his: 3 times\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "def extract_summary(sentences, tfidf_matrix, top_n=3):\n",
        "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "    ranked_sentences = [sentences[i] for i in sentence_scores.argsort()[::-1][:top_n]]\n",
        "    return ' '.join(ranked_sentences)\n",
        "\n",
        "summary = extract_summary(sentences, tfidf_matrix)\n",
        "\n",
        "\n",
        "def get_word_frequencies(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts.most_common(5)\n",
        "\n",
        "word_frequencies = get_word_frequencies(summary)\n",
        "\n",
        "\n",
        "print(\"Extracted Summary:\")\n",
        "print(summary, \"\\n\")\n",
        "\n",
        "print(\"Word Frequencies:\")\n",
        "for word, count in word_frequencies:\n",
        "    print(f\"{word}: {count} times\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4dc6adf",
      "metadata": {
        "id": "c4dc6adf"
      },
      "source": [
        "## Phase 4: LSTM-Based Summarization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "de382db9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de382db9",
        "outputId": "9db073f2-f4ac-4f2e-e53b-dcefd5ffb7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading files from category: business\n",
            "Reading files from category: entertainment\n",
            "Reading files from category: politics\n",
            "Reading files from category: sport\n",
            "Reading files from category: tech\n",
            "Reading files from category: business\n",
            "Reading files from category: entertainment\n",
            "Reading files from category: politics\n",
            "Reading files from category: sport\n",
            "Reading files from category: tech\n",
            "Epoch 1/10, Loss: 0.1672\n",
            "Epoch 2/10, Loss: 0.1594\n",
            "Epoch 3/10, Loss: 0.1519\n",
            "Epoch 4/10, Loss: 0.1443\n",
            "Epoch 5/10, Loss: 0.1366\n",
            "Epoch 6/10, Loss: 0.1286\n",
            "Epoch 7/10, Loss: 0.1203\n",
            "Epoch 8/10, Loss: 0.1116\n",
            "Epoch 9/10, Loss: 0.1027\n",
            "Epoch 10/10, Loss: 0.0937\n",
            "LSTM Model Training Complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import os\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Function to read files from folders\n",
        "def read_files_from_folder(folder_path):\n",
        "    data = []\n",
        "    categories = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "    \n",
        "    for category in categories:\n",
        "        category_path = os.path.join(folder_path, category)\n",
        "        if os.path.exists(category_path):\n",
        "            print(f\"Reading files from category: {category}\")\n",
        "            for filename in os.listdir(category_path):\n",
        "                file_path = os.path.join(category_path, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    try:\n",
        "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                            content = file.read()\n",
        "                    except UnicodeDecodeError:\n",
        "                        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "                            content = file.read()\n",
        "                    data.append((category, filename, content))\n",
        "        else:\n",
        "            print(f\"Category folder not found: {category_path}\")\n",
        "    return data\n",
        "\n",
        "# Paths to the folders\n",
        "articles_folder = 'C:/Users/Manish/OneDrive/Desktop/NLP/archive/BBC News Summary/News Articles'\n",
        "summaries_folder = 'C:/Users/Manish/OneDrive/Desktop/NLP/archive/BBC News Summary/Summaries'\n",
        "\n",
        "# Read the data\n",
        "articles_data = read_files_from_folder(articles_folder)\n",
        "summaries_data = read_files_from_folder(summaries_folder)\n",
        "\n",
        "# Create DataFrames\n",
        "df_articles = pd.DataFrame(articles_data, columns=['Category', 'Filename', 'Article'])\n",
        "df_summaries = pd.DataFrame(summaries_data, columns=['Category', 'Filename', 'Summary'])\n",
        "\n",
        "# Merge the DataFrames\n",
        "data = pd.merge(df_articles, df_summaries, on=['Category', 'Filename'])\n",
        "\n",
        "# Tokenization\n",
        "data['tokenized_articles'] = data['Article'].apply(lambda x: word_tokenize(x.lower()))\n",
        "data['tokenized_summaries'] = data['Summary'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(data['tokenized_articles'].tolist() + data['tokenized_summaries'].tolist(), \n",
        "                          vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to vectors\n",
        "def vectorize_sentence(sentence, model, vector_size=100):\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros((vector_size,))\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "data['article_vectors'] = data['tokenized_articles'].apply(lambda x: vectorize_sentence(x, word2vec_model))\n",
        "data['summary_vectors'] = data['tokenized_summaries'].apply(lambda x: vectorize_sentence(x, word2vec_model))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.vstack(data['article_vectors'].values)\n",
        "y = np.vstack(data['summary_vectors'].values)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMSummarizer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMSummarizer, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x.unsqueeze(1))\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "input_size = 100\n",
        "hidden_size = 128\n",
        "output_size = 100\n",
        "num_layers = 1\n",
        "\n",
        "model = LSTMSummarizer(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n",
        "    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.float32))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"LSTM Model Training Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1779467",
      "metadata": {
        "id": "d1779467"
      },
      "source": [
        "## Phase 5: Save Extracted Summary to File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5bf93fff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bf93fff",
        "outputId": "984ae371-18f5-4be6-e9aa-b1ea9c46f592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“„ Original text and summary saved to summary.txt\n"
          ]
        }
      ],
      "source": [
        "# Function to summarize unseen user input\n",
        "def summarize_text(input_text, model, word2vec_model, max_sentences=3):\n",
        "    tokenized_sentences = sent_tokenize(input_text)\n",
        "    tokenized_words = [word_tokenize(sent.lower()) for sent in tokenized_sentences]\n",
        "    sentence_vectors = np.array([vectorize_sentence(sent, word2vec_model) for sent in tokenized_words])\n",
        "\n",
        "    if sentence_vectors.shape[0] == 0:\n",
        "        return \"No meaningful sentences found for summarization.\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sentence_scores = model(torch.tensor(sentence_vectors, dtype=torch.float32)).squeeze().cpu().numpy()\n",
        "        \n",
        "        # Handle the case when sentence_scores is a single value\n",
        "        if np.isscalar(sentence_scores) or len(sentence_scores.shape) == 0:\n",
        "            ranked_indices = [0]\n",
        "        else:\n",
        "            # For multi-dimensional arrays, take the sum along the last axis\n",
        "            if len(sentence_scores.shape) > 1:\n",
        "                sentence_scores = np.sum(sentence_scores, axis=1)\n",
        "            \n",
        "            # Get the indices of the top-scoring sentences\n",
        "            ranked_indices = np.argsort(sentence_scores)[::-1][:min(len(tokenized_sentences), max_sentences)]\n",
        "\n",
        "    # Convert numpy array to list before sorting\n",
        "    ranked_indices_list = ranked_indices.tolist()\n",
        "    \n",
        "    # Extract sentences in their original order\n",
        "    ranked_sentences = [tokenized_sentences[i] for i in sorted(ranked_indices_list)]\n",
        "    formatted_summary = ' '.join(ranked_sentences)\n",
        "\n",
        "    return formatted_summary\n",
        "\n",
        "# Use user input from Phase 1\n",
        "try:\n",
        "    summarized_text = summarize_text(text, model, word2vec_model)\n",
        "\n",
        "    # Save summary to file\n",
        "    output_file = \"summary.txt\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"Original Text:\\n\")\n",
        "        file.write(text + \"\\n\\n\")\n",
        "        file.write(\"Summarized Text:\\n\")\n",
        "        file.write(summarized_text)\n",
        "\n",
        "    print(f\"ðŸ“„ Original text and summary saved to {output_file}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during summarization: {e}\")\n",
        "    print(\"Falling back to TF-IDF summary method.\")\n",
        "    \n",
        "    # Fallback to the TF-IDF method that was working earlier\n",
        "    summary = extract_summary(sentences, tfidf_matrix)\n",
        "    \n",
        "    output_file = \"summary.txt\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"Original Text:\\n\")\n",
        "        file.write(text + \"\\n\\n\")\n",
        "        file.write(\"Summarized Text (TF-IDF method):\\n\")\n",
        "        file.write(summary)\n",
        "    \n",
        "    print(f\"ðŸ“„ Original text and TF-IDF summary saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80064538",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All visualizations have been generated successfully!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "\n",
        "# Sample article and summary for demonstration\n",
        "original_text = text\n",
        "\n",
        "lstm_summary = summary\n",
        "\n",
        "# 1. Text Length Comparison\n",
        "def plot_text_length_comparison(original, summary):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Calculate lengths\n",
        "    original_words = len(word_tokenize(original))\n",
        "    summary_words = len(word_tokenize(summary))\n",
        "    original_sentences = len(sent_tokenize(original))\n",
        "    summary_sentences = len(sent_tokenize(summary))\n",
        "    \n",
        "    # Text compression rate\n",
        "    compression_rate = (1 - (summary_words / original_words)) * 100\n",
        "    \n",
        "    # Create bar chart\n",
        "    labels = ['Words', 'Sentences']\n",
        "    original_counts = [original_words, original_sentences]\n",
        "    summary_counts = [summary_words, summary_sentences]\n",
        "    \n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    rects1 = ax.bar(x - width/2, original_counts, width, label='Original Article')\n",
        "    rects2 = ax.bar(x + width/2, summary_counts, width, label='Summary')\n",
        "    \n",
        "    # Add labels, title and legend\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'Article vs Summary Comparison\\nCompression Rate: {compression_rate:.1f}%')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    \n",
        "    # Add count labels above bars\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height}',\n",
        "                        xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "                        \n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    plt.savefig('text_length_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return 'text_length_comparison.png'\n",
        "\n",
        "# 2. Word Cloud Comparison\n",
        "def generate_word_clouds(original, summary):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # Generate word clouds\n",
        "    wc1 = WordCloud(width=800, height=400, background_color='white', \n",
        "                   max_words=100, contour_width=3, contour_color='steelblue')\n",
        "    wc2 = WordCloud(width=800, height=400, background_color='white',\n",
        "                   max_words=100, contour_width=3, contour_color='steelblue')\n",
        "    \n",
        "    # Generate from original text\n",
        "    wc1.generate(original)\n",
        "    \n",
        "    # Generate from summary\n",
        "    wc2.generate(summary)\n",
        "    \n",
        "    # Display the word clouds\n",
        "    ax1.imshow(wc1, interpolation='bilinear')\n",
        "    ax1.set_title('Original Article', fontsize=20)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    ax2.imshow(wc2, interpolation='bilinear')\n",
        "    ax2.set_title('Summary', fontsize=20)\n",
        "    ax2.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('wordcloud_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return 'wordcloud_comparison.png'\n",
        "\n",
        "# 3. TF-IDF Term Importance\n",
        "def plot_tfidf_importance(original, summary):\n",
        "    # Create a corpus from the original and summary\n",
        "    corpus = [original, summary]\n",
        "    \n",
        "    # Create the TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Get TF-IDF scores\n",
        "    original_scores = tfidf_matrix[0].toarray()[0]\n",
        "    summary_scores = tfidf_matrix[1].toarray()[0]\n",
        "    \n",
        "    # Create a DataFrame for the scores\n",
        "    df = pd.DataFrame({\n",
        "        'term': feature_names,\n",
        "        'original_score': original_scores,\n",
        "        'summary_score': summary_scores\n",
        "    })\n",
        "    \n",
        "    # Calculate term importance in summary relative to original\n",
        "    df['importance'] = df['summary_score'] / (df['original_score'] + 0.0001)  # Avoid division by zero\n",
        "    \n",
        "    # Sort by importance (highest to lowest)\n",
        "    df = df.sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Get top terms that appear in both texts\n",
        "    top_terms = df[(df['original_score'] > 0) & (df['summary_score'] > 0)].head(15)\n",
        "    \n",
        "    # Create horizontal bar chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(top_terms['term'], top_terms['importance'], color='skyblue')\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.ylabel('Terms')\n",
        "    plt.title('Top Terms by Importance in Summary Relative to Original')\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('term_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return 'term_importance.png'\n",
        "\n",
        "# 4. Sentence Selection Heatmap\n",
        "def plot_sentence_selection(original, summary):\n",
        "    original_sentences = sent_tokenize(original)\n",
        "    summary_sentences = sent_tokenize(summary)\n",
        "    \n",
        "    # Create a matrix showing which original sentences were selected\n",
        "    selection_matrix = np.zeros((len(original_sentences), 1))\n",
        "    \n",
        "    # Mark which sentences were selected\n",
        "    for i, orig_sent in enumerate(original_sentences):\n",
        "        if orig_sent in summary_sentences:\n",
        "            selection_matrix[i, 0] = 1\n",
        "    \n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, len(original_sentences)/2))\n",
        "    ax = sns.heatmap(selection_matrix, cmap=['lightgray', 'forestgreen'], \n",
        "                     cbar=False, linewidths=.5, linecolor='gray')\n",
        "    plt.title('Sentence Selection Pattern')\n",
        "    plt.ylabel('Original Sentence Index')\n",
        "    plt.xlabel('Selected for Summary')\n",
        "    \n",
        "    # Add sentence texts as y-tick labels (shortened)\n",
        "    shortened_sentences = [s[:50] + '...' if len(s) > 50 else s for s in original_sentences]\n",
        "    plt.yticks(np.arange(len(original_sentences)) + 0.5, shortened_sentences, fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sentence_selection.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return 'sentence_selection.png'\n",
        "\n",
        "# 5. Model Performance Radar Chart\n",
        "def plot_performance_radar():\n",
        "    # Evaluation metrics (example scores - replace with actual evaluation)\n",
        "    categories = ['Content Coverage', 'Relevance', 'Coherence', 'Non-redundancy', 'Readability']\n",
        "    \n",
        "    # Example scores (0-1 scale)\n",
        "    tfidf_scores = [0.65, 0.70, 0.60, 0.75, 0.80]\n",
        "    lstm_scores = [0.75, 0.80, 0.65, 0.70, 0.85]\n",
        "    \n",
        "    # Number of categories\n",
        "    N = len(categories)\n",
        "    \n",
        "    # Create angle for each category\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]  # Close the loop\n",
        "    \n",
        "    # Add values for each method (also close the loop)\n",
        "    tfidf_scores += tfidf_scores[:1]\n",
        "    lstm_scores += lstm_scores[:1]\n",
        "    \n",
        "    # Initialize the figure\n",
        "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
        "    \n",
        "    # Draw one axis per variable and add labels\n",
        "    plt.xticks(angles[:-1], categories, fontsize=12)\n",
        "    \n",
        "    # Draw the outlines for each method\n",
        "    ax.plot(angles, tfidf_scores, linewidth=2, linestyle='solid', label='TF-IDF')\n",
        "    ax.plot(angles, lstm_scores, linewidth=2, linestyle='solid', label='LSTM')\n",
        "    \n",
        "    # Fill areas for each method\n",
        "    ax.fill(angles, tfidf_scores, alpha=0.25)\n",
        "    ax.fill(angles, lstm_scores, alpha=0.25)\n",
        "    \n",
        "    # Add legend\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "    \n",
        "    plt.title('Model Performance Comparison', fontsize=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_performance_radar.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return 'model_performance_radar.png'\n",
        "\n",
        "# Generate all visualizations\n",
        "text_chart = plot_text_length_comparison(original_text, lstm_summary)\n",
        "wordcloud = generate_word_clouds(original_text, lstm_summary)\n",
        "term_importance = plot_tfidf_importance(original_text, lstm_summary)\n",
        "sentence_selection = plot_sentence_selection(original_text, lstm_summary)\n",
        "performance_radar = plot_performance_radar()\n",
        "\n",
        "print(\"All visualizations have been generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6f7a001a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: nltk in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: torch in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.1.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: click in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\manish\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manish\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 1.8/8.1 MB 10.1 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 3.7/8.1 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 5.8/8.1 MB 9.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  7.9/8.1 MB 9.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.1/8.1 MB 9.6 MB/s eta 0:00:00\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
            "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------- ----------- 1.6/2.2 MB 9.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 9.5 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
            "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, wordcloud, seaborn\n",
            "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.1 seaborn-0.13.2 wordcloud-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas matplotlib seaborn nltk scikit-learn gensim torch wordcloud\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
